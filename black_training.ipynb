{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'omokdataset' from 'c:\\\\Users\\\\82105\\\\Desktop\\\\Omok\\\\omokdataset.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import omokdataset\n",
    "reload(omokdataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meta-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cpu device\n"
     ]
    }
   ],
   "source": [
    "# cuda or mps\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger\n",
    "class AverageMeter():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = 'omok_dataset'\n",
    "training_path = os.path.join('checkpoint', 'black.pth')\n",
    "who_win = 1 # 0: draw, 1: black, 2: white\n",
    "level = 0\n",
    "mode = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = omokdataset.OmokDataset(dataset_root, who_win, level, mode)\n",
    "\n",
    "split_ratio = 0.8\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "train_set, val_set = torch.utils.data.random_split(train_set, \n",
    "                                                   [int(len(train_set)*split_ratio), \n",
    "                                                    len(train_set)-int(len(train_set)*split_ratio)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import models.model\\nreload(models.model)\\n\\nmodel = models.model.SimpleNet().to(device)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import models.model\n",
    "reload(models.model)\n",
    "\n",
    "model = models.model.SimpleNet().to(device)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.blackmodel\n",
    "reload(models.blackmodel)\n",
    "\n",
    "model = models.blackmodel.BlackNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_meter = AverageMeter()\n",
    "tr_loss_meter = AverageMeter()\n",
    "rot_loss_meter = AverageMeter()\n",
    "\n",
    "train_loss_log = []\n",
    "train_tr_loss_log = []\n",
    "train_rot_loss_log = []\n",
    "val_loss_log = []\n",
    "val_tr_loss_log = []\n",
    "val_rot_loss_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)\n",
    "visualize_step = max(1, total_step // 10) ################### 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [1/100] -------------------\n",
      "Epoch [1/100], Step [1/1], Loss: 5.3754\n",
      "==> Train loss: 5.3754\n",
      "------------------- Val: Epoch [1/100] -------------------\n",
      "Epoch [1/100], Step [1/1], Loss: 5.4015\n",
      "==> Val loss: 5.4015\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [2/100] -------------------\n",
      "Epoch [2/100], Step [1/1], Loss: 5.3713\n",
      "==> Train loss: 5.3713\n",
      "------------------- Val: Epoch [2/100] -------------------\n",
      "Epoch [2/100], Step [1/1], Loss: 5.4014\n",
      "==> Val loss: 5.4014\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [3/100] -------------------\n",
      "Epoch [3/100], Step [1/1], Loss: 5.3672\n",
      "==> Train loss: 5.3672\n",
      "------------------- Val: Epoch [3/100] -------------------\n",
      "Epoch [3/100], Step [1/1], Loss: 5.4014\n",
      "==> Val loss: 5.4014\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [4/100] -------------------\n",
      "Epoch [4/100], Step [1/1], Loss: 5.3632\n",
      "==> Train loss: 5.3632\n",
      "------------------- Val: Epoch [4/100] -------------------\n",
      "Epoch [4/100], Step [1/1], Loss: 5.4013\n",
      "==> Val loss: 5.4013\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [5/100] -------------------\n",
      "Epoch [5/100], Step [1/1], Loss: 5.3592\n",
      "==> Train loss: 5.3592\n",
      "------------------- Val: Epoch [5/100] -------------------\n",
      "Epoch [5/100], Step [1/1], Loss: 5.4012\n",
      "==> Val loss: 5.4012\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [6/100] -------------------\n",
      "Epoch [6/100], Step [1/1], Loss: 5.3552\n",
      "==> Train loss: 5.3552\n",
      "------------------- Val: Epoch [6/100] -------------------\n",
      "Epoch [6/100], Step [1/1], Loss: 5.4011\n",
      "==> Val loss: 5.4011\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [7/100] -------------------\n",
      "Epoch [7/100], Step [1/1], Loss: 5.3512\n",
      "==> Train loss: 5.3512\n",
      "------------------- Val: Epoch [7/100] -------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Step [1/1], Loss: 5.4010\n",
      "==> Val loss: 5.4010\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [8/100] -------------------\n",
      "Epoch [8/100], Step [1/1], Loss: 5.3472\n",
      "==> Train loss: 5.3472\n",
      "------------------- Val: Epoch [8/100] -------------------\n",
      "Epoch [8/100], Step [1/1], Loss: 5.4008\n",
      "==> Val loss: 5.4008\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [9/100] -------------------\n",
      "Epoch [9/100], Step [1/1], Loss: 5.3432\n",
      "==> Train loss: 5.3432\n",
      "------------------- Val: Epoch [9/100] -------------------\n",
      "Epoch [9/100], Step [1/1], Loss: 5.4006\n",
      "==> Val loss: 5.4006\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [10/100] -------------------\n",
      "Epoch [10/100], Step [1/1], Loss: 5.3390\n",
      "==> Train loss: 5.3390\n",
      "------------------- Val: Epoch [10/100] -------------------\n",
      "Epoch [10/100], Step [1/1], Loss: 5.4004\n",
      "==> Val loss: 5.4004\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [11/100] -------------------\n",
      "Epoch [11/100], Step [1/1], Loss: 5.3348\n",
      "==> Train loss: 5.3348\n",
      "------------------- Val: Epoch [11/100] -------------------\n",
      "Epoch [11/100], Step [1/1], Loss: 5.4001\n",
      "==> Val loss: 5.4001\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [12/100] -------------------\n",
      "Epoch [12/100], Step [1/1], Loss: 5.3305\n",
      "==> Train loss: 5.3305\n",
      "------------------- Val: Epoch [12/100] -------------------\n",
      "Epoch [12/100], Step [1/1], Loss: 5.3997\n",
      "==> Val loss: 5.3997\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [13/100] -------------------\n",
      "Epoch [13/100], Step [1/1], Loss: 5.3261\n",
      "==> Train loss: 5.3261\n",
      "------------------- Val: Epoch [13/100] -------------------\n",
      "Epoch [13/100], Step [1/1], Loss: 5.3992\n",
      "==> Val loss: 5.3992\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [14/100] -------------------\n",
      "Epoch [14/100], Step [1/1], Loss: 5.3215\n",
      "==> Train loss: 5.3215\n",
      "------------------- Val: Epoch [14/100] -------------------\n",
      "Epoch [14/100], Step [1/1], Loss: 5.3987\n",
      "==> Val loss: 5.3987\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [15/100] -------------------\n",
      "Epoch [15/100], Step [1/1], Loss: 5.3168\n",
      "==> Train loss: 5.3168\n",
      "------------------- Val: Epoch [15/100] -------------------\n",
      "Epoch [15/100], Step [1/1], Loss: 5.3982\n",
      "==> Val loss: 5.3982\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [16/100] -------------------\n",
      "Epoch [16/100], Step [1/1], Loss: 5.3119\n",
      "==> Train loss: 5.3119\n",
      "------------------- Val: Epoch [16/100] -------------------\n",
      "Epoch [16/100], Step [1/1], Loss: 5.3977\n",
      "==> Val loss: 5.3977\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [17/100] -------------------\n",
      "Epoch [17/100], Step [1/1], Loss: 5.3068\n",
      "==> Train loss: 5.3068\n",
      "------------------- Val: Epoch [17/100] -------------------\n",
      "Epoch [17/100], Step [1/1], Loss: 5.3972\n",
      "==> Val loss: 5.3972\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [18/100] -------------------\n",
      "Epoch [18/100], Step [1/1], Loss: 5.3015\n",
      "==> Train loss: 5.3015\n",
      "------------------- Val: Epoch [18/100] -------------------\n",
      "Epoch [18/100], Step [1/1], Loss: 5.3967\n",
      "==> Val loss: 5.3967\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [19/100] -------------------\n",
      "Epoch [19/100], Step [1/1], Loss: 5.2958\n",
      "==> Train loss: 5.2958\n",
      "------------------- Val: Epoch [19/100] -------------------\n",
      "Epoch [19/100], Step [1/1], Loss: 5.3960\n",
      "==> Val loss: 5.3960\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [20/100] -------------------\n",
      "Epoch [20/100], Step [1/1], Loss: 5.2900\n",
      "==> Train loss: 5.2900\n",
      "------------------- Val: Epoch [20/100] -------------------\n",
      "Epoch [20/100], Step [1/1], Loss: 5.3953\n",
      "==> Val loss: 5.3953\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [21/100] -------------------\n",
      "Epoch [21/100], Step [1/1], Loss: 5.2839\n",
      "==> Train loss: 5.2839\n",
      "------------------- Val: Epoch [21/100] -------------------\n",
      "Epoch [21/100], Step [1/1], Loss: 5.3946\n",
      "==> Val loss: 5.3946\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [22/100] -------------------\n",
      "Epoch [22/100], Step [1/1], Loss: 5.2773\n",
      "==> Train loss: 5.2773\n",
      "------------------- Val: Epoch [22/100] -------------------\n",
      "Epoch [22/100], Step [1/1], Loss: 5.3940\n",
      "==> Val loss: 5.3940\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [23/100] -------------------\n",
      "Epoch [23/100], Step [1/1], Loss: 5.2702\n",
      "==> Train loss: 5.2702\n",
      "------------------- Val: Epoch [23/100] -------------------\n",
      "Epoch [23/100], Step [1/1], Loss: 5.3934\n",
      "==> Val loss: 5.3934\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [24/100] -------------------\n",
      "Epoch [24/100], Step [1/1], Loss: 5.2622\n",
      "==> Train loss: 5.2622\n",
      "------------------- Val: Epoch [24/100] -------------------\n",
      "Epoch [24/100], Step [1/1], Loss: 5.3926\n",
      "==> Val loss: 5.3926\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [25/100] -------------------\n",
      "Epoch [25/100], Step [1/1], Loss: 5.2536\n",
      "==> Train loss: 5.2536\n",
      "------------------- Val: Epoch [25/100] -------------------\n",
      "Epoch [25/100], Step [1/1], Loss: 5.3918\n",
      "==> Val loss: 5.3918\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [26/100] -------------------\n",
      "Epoch [26/100], Step [1/1], Loss: 5.2442\n",
      "==> Train loss: 5.2442\n",
      "------------------- Val: Epoch [26/100] -------------------\n",
      "Epoch [26/100], Step [1/1], Loss: 5.3912\n",
      "==> Val loss: 5.3912\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [27/100] -------------------\n",
      "Epoch [27/100], Step [1/1], Loss: 5.2338\n",
      "==> Train loss: 5.2338\n",
      "------------------- Val: Epoch [27/100] -------------------\n",
      "Epoch [27/100], Step [1/1], Loss: 5.3905\n",
      "==> Val loss: 5.3905\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [28/100] -------------------\n",
      "Epoch [28/100], Step [1/1], Loss: 5.2221\n",
      "==> Train loss: 5.2221\n",
      "------------------- Val: Epoch [28/100] -------------------\n",
      "Epoch [28/100], Step [1/1], Loss: 5.3898\n",
      "==> Val loss: 5.3898\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [29/100] -------------------\n",
      "Epoch [29/100], Step [1/1], Loss: 5.2086\n",
      "==> Train loss: 5.2086\n",
      "------------------- Val: Epoch [29/100] -------------------\n",
      "Epoch [29/100], Step [1/1], Loss: 5.3889\n",
      "==> Val loss: 5.3889\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [30/100] -------------------\n",
      "Epoch [30/100], Step [1/1], Loss: 5.1934\n",
      "==> Train loss: 5.1934\n",
      "------------------- Val: Epoch [30/100] -------------------\n",
      "Epoch [30/100], Step [1/1], Loss: 5.3881\n",
      "==> Val loss: 5.3881\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [31/100] -------------------\n",
      "Epoch [31/100], Step [1/1], Loss: 5.1763\n",
      "==> Train loss: 5.1763\n",
      "------------------- Val: Epoch [31/100] -------------------\n",
      "Epoch [31/100], Step [1/1], Loss: 5.3873\n",
      "==> Val loss: 5.3873\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [32/100] -------------------\n",
      "Epoch [32/100], Step [1/1], Loss: 5.1569\n",
      "==> Train loss: 5.1569\n",
      "------------------- Val: Epoch [32/100] -------------------\n",
      "Epoch [32/100], Step [1/1], Loss: 5.3862\n",
      "==> Val loss: 5.3862\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [33/100] -------------------\n",
      "Epoch [33/100], Step [1/1], Loss: 5.1346\n",
      "==> Train loss: 5.1346\n",
      "------------------- Val: Epoch [33/100] -------------------\n",
      "Epoch [33/100], Step [1/1], Loss: 5.3849\n",
      "==> Val loss: 5.3849\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [34/100] -------------------\n",
      "Epoch [34/100], Step [1/1], Loss: 5.1091\n",
      "==> Train loss: 5.1091\n",
      "------------------- Val: Epoch [34/100] -------------------\n",
      "Epoch [34/100], Step [1/1], Loss: 5.3835\n",
      "==> Val loss: 5.3835\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [35/100] -------------------\n",
      "Epoch [35/100], Step [1/1], Loss: 5.0797\n",
      "==> Train loss: 5.0797\n",
      "------------------- Val: Epoch [35/100] -------------------\n",
      "Epoch [35/100], Step [1/1], Loss: 5.3821\n",
      "==> Val loss: 5.3821\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [36/100] -------------------\n",
      "Epoch [36/100], Step [1/1], Loss: 5.0454\n",
      "==> Train loss: 5.0454\n",
      "------------------- Val: Epoch [36/100] -------------------\n",
      "Epoch [36/100], Step [1/1], Loss: 5.3805\n",
      "==> Val loss: 5.3805\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [37/100] -------------------\n",
      "Epoch [37/100], Step [1/1], Loss: 5.0056\n",
      "==> Train loss: 5.0056\n",
      "------------------- Val: Epoch [37/100] -------------------\n",
      "Epoch [37/100], Step [1/1], Loss: 5.3789\n",
      "==> Val loss: 5.3789\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [38/100] -------------------\n",
      "Epoch [38/100], Step [1/1], Loss: 4.9593\n",
      "==> Train loss: 4.9593\n",
      "------------------- Val: Epoch [38/100] -------------------\n",
      "Epoch [38/100], Step [1/1], Loss: 5.3772\n",
      "==> Val loss: 5.3772\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [39/100] -------------------\n",
      "Epoch [39/100], Step [1/1], Loss: 4.9049\n",
      "==> Train loss: 4.9049\n",
      "------------------- Val: Epoch [39/100] -------------------\n",
      "Epoch [39/100], Step [1/1], Loss: 5.3754\n",
      "==> Val loss: 5.3754\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [40/100] -------------------\n",
      "Epoch [40/100], Step [1/1], Loss: 4.8407\n",
      "==> Train loss: 4.8407\n",
      "------------------- Val: Epoch [40/100] -------------------\n",
      "Epoch [40/100], Step [1/1], Loss: 5.3730\n",
      "==> Val loss: 5.3730\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [41/100] -------------------\n",
      "Epoch [41/100], Step [1/1], Loss: 4.7654\n",
      "==> Train loss: 4.7654\n",
      "------------------- Val: Epoch [41/100] -------------------\n",
      "Epoch [41/100], Step [1/1], Loss: 5.3702\n",
      "==> Val loss: 5.3702\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [42/100] -------------------\n",
      "Epoch [42/100], Step [1/1], Loss: 4.6769\n",
      "==> Train loss: 4.6769\n",
      "------------------- Val: Epoch [42/100] -------------------\n",
      "Epoch [42/100], Step [1/1], Loss: 5.3679\n",
      "==> Val loss: 5.3679\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [43/100] -------------------\n",
      "Epoch [43/100], Step [1/1], Loss: 4.5730\n",
      "==> Train loss: 4.5730\n",
      "------------------- Val: Epoch [43/100] -------------------\n",
      "Epoch [43/100], Step [1/1], Loss: 5.3652\n",
      "==> Val loss: 5.3652\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [44/100] -------------------\n",
      "Epoch [44/100], Step [1/1], Loss: 4.4511\n",
      "==> Train loss: 4.4511\n",
      "------------------- Val: Epoch [44/100] -------------------\n",
      "Epoch [44/100], Step [1/1], Loss: 5.3625\n",
      "==> Val loss: 5.3625\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [45/100] -------------------\n",
      "Epoch [45/100], Step [1/1], Loss: 4.3085\n",
      "==> Train loss: 4.3085\n",
      "------------------- Val: Epoch [45/100] -------------------\n",
      "Epoch [45/100], Step [1/1], Loss: 5.3598\n",
      "==> Val loss: 5.3598\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [46/100] -------------------\n",
      "Epoch [46/100], Step [1/1], Loss: 4.1419\n",
      "==> Train loss: 4.1419\n",
      "------------------- Val: Epoch [46/100] -------------------\n",
      "Epoch [46/100], Step [1/1], Loss: 5.3570\n",
      "==> Val loss: 5.3570\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [47/100] -------------------\n",
      "Epoch [47/100], Step [1/1], Loss: 3.9472\n",
      "==> Train loss: 3.9472\n",
      "------------------- Val: Epoch [47/100] -------------------\n",
      "Epoch [47/100], Step [1/1], Loss: 5.3543\n",
      "==> Val loss: 5.3543\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [48/100] -------------------\n",
      "Epoch [48/100], Step [1/1], Loss: 3.7204\n",
      "==> Train loss: 3.7204\n",
      "------------------- Val: Epoch [48/100] -------------------\n",
      "Epoch [48/100], Step [1/1], Loss: 5.3526\n",
      "==> Val loss: 5.3526\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [49/100] -------------------\n",
      "Epoch [49/100], Step [1/1], Loss: 3.4577\n",
      "==> Train loss: 3.4577\n",
      "------------------- Val: Epoch [49/100] -------------------\n",
      "Epoch [49/100], Step [1/1], Loss: 5.3523\n",
      "==> Val loss: 5.3523\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [50/100] -------------------\n",
      "Epoch [50/100], Step [1/1], Loss: 3.1556\n",
      "==> Train loss: 3.1556\n",
      "------------------- Val: Epoch [50/100] -------------------\n",
      "Epoch [50/100], Step [1/1], Loss: 5.3546\n",
      "==> Val loss: 5.3546\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [51/100] -------------------\n",
      "Epoch [51/100], Step [1/1], Loss: 2.8117\n",
      "==> Train loss: 2.8117\n",
      "------------------- Val: Epoch [51/100] -------------------\n",
      "Epoch [51/100], Step [1/1], Loss: 5.3612\n",
      "==> Val loss: 5.3612\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [52/100] -------------------\n",
      "Epoch [52/100], Step [1/1], Loss: 2.4287\n",
      "==> Train loss: 2.4287\n",
      "------------------- Val: Epoch [52/100] -------------------\n",
      "Epoch [52/100], Step [1/1], Loss: 5.3758\n",
      "==> Val loss: 5.3758\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [53/100] -------------------\n",
      "Epoch [53/100], Step [1/1], Loss: 2.0188\n",
      "==> Train loss: 2.0188\n",
      "------------------- Val: Epoch [53/100] -------------------\n",
      "Epoch [53/100], Step [1/1], Loss: 5.4049\n",
      "==> Val loss: 5.4049\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [54/100] -------------------\n",
      "Epoch [54/100], Step [1/1], Loss: 1.6094\n",
      "==> Train loss: 1.6094\n",
      "------------------- Val: Epoch [54/100] -------------------\n",
      "Epoch [54/100], Step [1/1], Loss: 5.4592\n",
      "==> Val loss: 5.4592\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [55/100] -------------------\n",
      "Epoch [55/100], Step [1/1], Loss: 1.2467\n",
      "==> Train loss: 1.2467\n",
      "------------------- Val: Epoch [55/100] -------------------\n",
      "Epoch [55/100], Step [1/1], Loss: 5.5561\n",
      "==> Val loss: 5.5561\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [56/100] -------------------\n",
      "Epoch [56/100], Step [1/1], Loss: 0.9804\n",
      "==> Train loss: 0.9804\n",
      "------------------- Val: Epoch [56/100] -------------------\n",
      "Epoch [56/100], Step [1/1], Loss: 5.7233\n",
      "==> Val loss: 5.7233\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [57/100] -------------------\n",
      "Epoch [57/100], Step [1/1], Loss: 0.8257\n",
      "==> Train loss: 0.8257\n",
      "------------------- Val: Epoch [57/100] -------------------\n",
      "Epoch [57/100], Step [1/1], Loss: 5.9882\n",
      "==> Val loss: 5.9882\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [58/100] -------------------\n",
      "Epoch [58/100], Step [1/1], Loss: 0.7526\n",
      "==> Train loss: 0.7526\n",
      "------------------- Val: Epoch [58/100] -------------------\n",
      "Epoch [58/100], Step [1/1], Loss: 6.3649\n",
      "==> Val loss: 6.3649\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [59/100] -------------------\n",
      "Epoch [59/100], Step [1/1], Loss: 0.7205\n",
      "==> Train loss: 0.7205\n",
      "------------------- Val: Epoch [59/100] -------------------\n",
      "Epoch [59/100], Step [1/1], Loss: 6.8422\n",
      "==> Val loss: 6.8422\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [60/100] -------------------\n",
      "Epoch [60/100], Step [1/1], Loss: 0.7046\n",
      "==> Train loss: 0.7046\n",
      "------------------- Val: Epoch [60/100] -------------------\n",
      "Epoch [60/100], Step [1/1], Loss: 7.3929\n",
      "==> Val loss: 7.3929\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [61/100] -------------------\n",
      "Epoch [61/100], Step [1/1], Loss: 0.6957\n",
      "==> Train loss: 0.6957\n",
      "------------------- Val: Epoch [61/100] -------------------\n",
      "Epoch [61/100], Step [1/1], Loss: 7.9883\n",
      "==> Val loss: 7.9883\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [62/100] -------------------\n",
      "Epoch [62/100], Step [1/1], Loss: 0.6928\n",
      "==> Train loss: 0.6928\n",
      "------------------- Val: Epoch [62/100] -------------------\n",
      "Epoch [62/100], Step [1/1], Loss: 8.6043\n",
      "==> Val loss: 8.6043\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [63/100] -------------------\n",
      "Epoch [63/100], Step [1/1], Loss: 0.6963\n",
      "==> Train loss: 0.6963\n",
      "------------------- Val: Epoch [63/100] -------------------\n",
      "Epoch [63/100], Step [1/1], Loss: 9.2220\n",
      "==> Val loss: 9.2220\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [64/100] -------------------\n",
      "Epoch [64/100], Step [1/1], Loss: 0.7024\n",
      "==> Train loss: 0.7024\n",
      "------------------- Val: Epoch [64/100] -------------------\n",
      "Epoch [64/100], Step [1/1], Loss: 9.8253\n",
      "==> Val loss: 9.8253\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [65/100] -------------------\n",
      "Epoch [65/100], Step [1/1], Loss: 0.7048\n",
      "==> Train loss: 0.7048\n",
      "------------------- Val: Epoch [65/100] -------------------\n",
      "Epoch [65/100], Step [1/1], Loss: 10.4041\n",
      "==> Val loss: 10.4041\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [66/100] -------------------\n",
      "Epoch [66/100], Step [1/1], Loss: 0.7010\n",
      "==> Train loss: 0.7010\n",
      "------------------- Val: Epoch [66/100] -------------------\n",
      "Epoch [66/100], Step [1/1], Loss: 10.9566\n",
      "==> Val loss: 10.9566\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [67/100] -------------------\n",
      "Epoch [67/100], Step [1/1], Loss: 0.6945\n",
      "==> Train loss: 0.6945\n",
      "------------------- Val: Epoch [67/100] -------------------\n",
      "Epoch [67/100], Step [1/1], Loss: 11.4844\n",
      "==> Val loss: 11.4844\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [68/100] -------------------\n",
      "Epoch [68/100], Step [1/1], Loss: 0.6920\n",
      "==> Train loss: 0.6920\n",
      "------------------- Val: Epoch [68/100] -------------------\n",
      "Epoch [68/100], Step [1/1], Loss: 11.9874\n",
      "==> Val loss: 11.9874\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [69/100] -------------------\n",
      "Epoch [69/100], Step [1/1], Loss: 0.6959\n",
      "==> Train loss: 0.6959\n",
      "------------------- Val: Epoch [69/100] -------------------\n",
      "Epoch [69/100], Step [1/1], Loss: 12.4613\n",
      "==> Val loss: 12.4613\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [70/100] -------------------\n",
      "Epoch [70/100], Step [1/1], Loss: 0.7006\n",
      "==> Train loss: 0.7006\n",
      "------------------- Val: Epoch [70/100] -------------------\n",
      "Epoch [70/100], Step [1/1], Loss: 12.9029\n",
      "==> Val loss: 12.9029\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [71/100] -------------------\n",
      "Epoch [71/100], Step [1/1], Loss: 0.6994\n",
      "==> Train loss: 0.6994\n",
      "------------------- Val: Epoch [71/100] -------------------\n",
      "Epoch [71/100], Step [1/1], Loss: 13.3129\n",
      "==> Val loss: 13.3129\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [72/100] -------------------\n",
      "Epoch [72/100], Step [1/1], Loss: 0.6941\n",
      "==> Train loss: 0.6941\n",
      "------------------- Val: Epoch [72/100] -------------------\n",
      "Epoch [72/100], Step [1/1], Loss: 13.6960\n",
      "==> Val loss: 13.6960\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [73/100] -------------------\n",
      "Epoch [73/100], Step [1/1], Loss: 0.6919\n",
      "==> Train loss: 0.6919\n",
      "------------------- Val: Epoch [73/100] -------------------\n",
      "Epoch [73/100], Step [1/1], Loss: 14.0542\n",
      "==> Val loss: 14.0542\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [74/100] -------------------\n",
      "Epoch [74/100], Step [1/1], Loss: 0.6952\n",
      "==> Train loss: 0.6952\n",
      "------------------- Val: Epoch [74/100] -------------------\n",
      "Epoch [74/100], Step [1/1], Loss: 14.3850\n",
      "==> Val loss: 14.3850\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [75/100] -------------------\n",
      "Epoch [75/100], Step [1/1], Loss: 0.6981\n",
      "==> Train loss: 0.6981\n",
      "------------------- Val: Epoch [75/100] -------------------\n",
      "Epoch [75/100], Step [1/1], Loss: 14.6866\n",
      "==> Val loss: 14.6866\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [76/100] -------------------\n",
      "Epoch [76/100], Step [1/1], Loss: 0.6960\n",
      "==> Train loss: 0.6960\n",
      "------------------- Val: Epoch [76/100] -------------------\n",
      "Epoch [76/100], Step [1/1], Loss: 14.9624\n",
      "==> Val loss: 14.9624\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [77/100] -------------------\n",
      "Epoch [77/100], Step [1/1], Loss: 0.6923\n",
      "==> Train loss: 0.6923\n",
      "------------------- Val: Epoch [77/100] -------------------\n",
      "Epoch [77/100], Step [1/1], Loss: 15.2175\n",
      "==> Val loss: 15.2175\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [78/100] -------------------\n",
      "Epoch [78/100], Step [1/1], Loss: 0.6926\n",
      "==> Train loss: 0.6926\n",
      "------------------- Val: Epoch [78/100] -------------------\n",
      "Epoch [78/100], Step [1/1], Loss: 15.4523\n",
      "==> Val loss: 15.4523\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [79/100] -------------------\n",
      "Epoch [79/100], Step [1/1], Loss: 0.6955\n",
      "==> Train loss: 0.6955\n",
      "------------------- Val: Epoch [79/100] -------------------\n",
      "Epoch [79/100], Step [1/1], Loss: 15.6667\n",
      "==> Val loss: 15.6667\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [80/100] -------------------\n",
      "Epoch [80/100], Step [1/1], Loss: 0.6955\n",
      "==> Train loss: 0.6955\n",
      "------------------- Val: Epoch [80/100] -------------------\n",
      "Epoch [80/100], Step [1/1], Loss: 15.8621\n",
      "==> Val loss: 15.8621\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [81/100] -------------------\n",
      "Epoch [81/100], Step [1/1], Loss: 0.6928\n",
      "==> Train loss: 0.6928\n",
      "------------------- Val: Epoch [81/100] -------------------\n",
      "Epoch [81/100], Step [1/1], Loss: 16.0418\n",
      "==> Val loss: 16.0418\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [82/100] -------------------\n",
      "Epoch [82/100], Step [1/1], Loss: 0.6919\n",
      "==> Train loss: 0.6919\n",
      "------------------- Val: Epoch [82/100] -------------------\n",
      "Epoch [82/100], Step [1/1], Loss: 16.2069\n",
      "==> Val loss: 16.2069\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [83/100] -------------------\n",
      "Epoch [83/100], Step [1/1], Loss: 0.6938\n",
      "==> Train loss: 0.6938\n",
      "------------------- Val: Epoch [83/100] -------------------\n",
      "Epoch [83/100], Step [1/1], Loss: 16.3560\n",
      "==> Val loss: 16.3560\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [84/100] -------------------\n",
      "Epoch [84/100], Step [1/1], Loss: 0.6945\n",
      "==> Train loss: 0.6945\n",
      "------------------- Val: Epoch [84/100] -------------------\n",
      "Epoch [84/100], Step [1/1], Loss: 16.4896\n",
      "==> Val loss: 16.4896\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [85/100] -------------------\n",
      "Epoch [85/100], Step [1/1], Loss: 0.6927\n",
      "==> Train loss: 0.6927\n",
      "------------------- Val: Epoch [85/100] -------------------\n",
      "Epoch [85/100], Step [1/1], Loss: 16.6107\n",
      "==> Val loss: 16.6107\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [86/100] -------------------\n",
      "Epoch [86/100], Step [1/1], Loss: 0.6917\n",
      "==> Train loss: 0.6917\n",
      "------------------- Val: Epoch [86/100] -------------------\n",
      "Epoch [86/100], Step [1/1], Loss: 16.7219\n",
      "==> Val loss: 16.7219\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [87/100] -------------------\n",
      "Epoch [87/100], Step [1/1], Loss: 0.6929\n",
      "==> Train loss: 0.6929\n",
      "------------------- Val: Epoch [87/100] -------------------\n",
      "Epoch [87/100], Step [1/1], Loss: 16.8232\n",
      "==> Val loss: 16.8232\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [88/100] -------------------\n",
      "Epoch [88/100], Step [1/1], Loss: 0.6936\n",
      "==> Train loss: 0.6936\n",
      "------------------- Val: Epoch [88/100] -------------------\n",
      "Epoch [88/100], Step [1/1], Loss: 16.9149\n",
      "==> Val loss: 16.9149\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [89/100] -------------------\n",
      "Epoch [89/100], Step [1/1], Loss: 0.6924\n",
      "==> Train loss: 0.6924\n",
      "------------------- Val: Epoch [89/100] -------------------\n",
      "Epoch [89/100], Step [1/1], Loss: 16.9987\n",
      "==> Val loss: 16.9987\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [90/100] -------------------\n",
      "Epoch [90/100], Step [1/1], Loss: 0.6917\n",
      "==> Train loss: 0.6917\n",
      "------------------- Val: Epoch [90/100] -------------------\n",
      "Epoch [90/100], Step [1/1], Loss: 17.0754\n",
      "==> Val loss: 17.0754\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [91/100] -------------------\n",
      "Epoch [91/100], Step [1/1], Loss: 0.6925\n",
      "==> Train loss: 0.6925\n",
      "------------------- Val: Epoch [91/100] -------------------\n",
      "Epoch [91/100], Step [1/1], Loss: 17.1445\n",
      "==> Val loss: 17.1445\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [92/100] -------------------\n",
      "Epoch [92/100], Step [1/1], Loss: 0.6929\n",
      "==> Train loss: 0.6929\n",
      "------------------- Val: Epoch [92/100] -------------------\n",
      "Epoch [92/100], Step [1/1], Loss: 17.2057\n",
      "==> Val loss: 17.2057\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [93/100] -------------------\n",
      "Epoch [93/100], Step [1/1], Loss: 0.6921\n",
      "==> Train loss: 0.6921\n",
      "------------------- Val: Epoch [93/100] -------------------\n",
      "Epoch [93/100], Step [1/1], Loss: 17.2606\n",
      "==> Val loss: 17.2606\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [94/100] -------------------\n",
      "Epoch [94/100], Step [1/1], Loss: 0.6916\n",
      "==> Train loss: 0.6916\n",
      "------------------- Val: Epoch [94/100] -------------------\n",
      "Epoch [94/100], Step [1/1], Loss: 17.3112\n",
      "==> Val loss: 17.3112\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [95/100] -------------------\n",
      "Epoch [95/100], Step [1/1], Loss: 0.6922\n",
      "==> Train loss: 0.6922\n",
      "------------------- Val: Epoch [95/100] -------------------\n",
      "Epoch [95/100], Step [1/1], Loss: 17.3572\n",
      "==> Val loss: 17.3572\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [96/100] -------------------\n",
      "Epoch [96/100], Step [1/1], Loss: 0.6924\n",
      "==> Train loss: 0.6924\n",
      "------------------- Val: Epoch [96/100] -------------------\n",
      "Epoch [96/100], Step [1/1], Loss: 17.3990\n",
      "==> Val loss: 17.3990\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [97/100] -------------------\n",
      "Epoch [97/100], Step [1/1], Loss: 0.6918\n",
      "==> Train loss: 0.6918\n",
      "------------------- Val: Epoch [97/100] -------------------\n",
      "Epoch [97/100], Step [1/1], Loss: 17.4372\n",
      "==> Val loss: 17.4372\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [98/100] -------------------\n",
      "Epoch [98/100], Step [1/1], Loss: 0.6915\n",
      "==> Train loss: 0.6915\n",
      "------------------- Val: Epoch [98/100] -------------------\n",
      "Epoch [98/100], Step [1/1], Loss: 17.4718\n",
      "==> Val loss: 17.4718\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [99/100] -------------------\n",
      "Epoch [99/100], Step [1/1], Loss: 0.6920\n",
      "==> Train loss: 0.6920\n",
      "------------------- Val: Epoch [99/100] -------------------\n",
      "Epoch [99/100], Step [1/1], Loss: 17.5023\n",
      "==> Val loss: 17.5023\n",
      "learing rage:  0.0001\n",
      "------------------- Train: Epoch [100/100] -------------------\n",
      "Epoch [100/100], Step [1/1], Loss: 0.6920\n",
      "==> Train loss: 0.6920\n",
      "------------------- Val: Epoch [100/100] -------------------\n",
      "Epoch [100/100], Step [1/1], Loss: 17.5290\n",
      "==> Val loss: 17.5290\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = 1e10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print('learing rage: ', param_group['lr'])\n",
    "\n",
    "    # train\n",
    "    model.train()\n",
    "    print ('------------------- Train: Epoch [{}/{}] -------------------'.format(\\\n",
    "        epoch+1, num_epochs) )\n",
    "\n",
    "    loss_meter.reset()\n",
    "\n",
    "    for i, (board_status, target) in enumerate(train_loader):\n",
    "        board_status = board_status.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        board_status = board_status.type(torch.float32)\n",
    "        pred = model(board_status)\n",
    "        loss = loss_fn(pred, target)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # logging\n",
    "        loss_meter.update(loss.item(), board_status.size()[0] )\n",
    "\n",
    "        if (i+1) % visualize_step == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\\\n",
    "                epoch+1, num_epochs, i+1, total_step, loss.item() ) )\n",
    "\n",
    "    print ('==> Train loss: {:.4f}'.format(loss_meter.avg) )\n",
    "\n",
    "    train_loss_log.append(loss_meter.avg)\n",
    "\n",
    "    # val\n",
    "    model.eval() \n",
    "    print ('------------------- Val: Epoch [{}/{}] -------------------'.format(\\\n",
    "        epoch+1, num_epochs) ) \n",
    "    \n",
    "    loss_meter.reset()\n",
    "\n",
    "    for i, (board_status, target) in enumerate(val_loader):\n",
    "        board_status = board_status.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        board_status = board_status.type(torch.float32)\n",
    "        pred = model(board_status)\n",
    "        loss = loss_fn(pred, target)\n",
    "\n",
    "        # logging\n",
    "        loss_meter.update(loss.item(), board_status.size()[0] )\n",
    "\n",
    "        if (i+1) % visualize_step == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\\\n",
    "                epoch+1, num_epochs, i+1, total_step, loss.item() ) )\n",
    "            \n",
    "    print ('==> Val loss: {:.4f}'.format(loss_meter.avg) )\n",
    "\n",
    "    val_loss_log.append(loss_meter.avg)\n",
    "    # save model\n",
    "    if loss_meter.avg < best_val_loss:\n",
    "        best_val_loss = loss_meter.avg\n",
    "        if not os.path.exists('checkpoint'):\n",
    "            os.makedirs('checkpoint')\n",
    "        torch.save(model.state_dict(), 'checkpoint/best.pth')\n",
    "\n",
    "# save best model\n",
    "if not os.path.exists(training_path):\n",
    "    print('best model updated')\n",
    "    checkpoint = {\n",
    "        'model': torch.load('checkpoint/best.pth'),\n",
    "        'epoch': epoch,\n",
    "        'loss': best_val_loss\n",
    "    }\n",
    "    torch.save(checkpoint, training_path)\n",
    "else:\n",
    "    checkpoint = torch.load(training_path)\n",
    "    if checkpoint['loss'] > best_val_loss:\n",
    "        print('best model updated')\n",
    "        checkpoint = {\n",
    "            'model': torch.load('checkpoint/best.pth'),\n",
    "            'epoch': epoch,\n",
    "            'loss': best_val_loss\n",
    "        }\n",
    "        torch.save(checkpoint, training_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "plt.plot(train_loss_log, label='train loss')\n",
    "plt.plot(val_loss_log, label='val loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
